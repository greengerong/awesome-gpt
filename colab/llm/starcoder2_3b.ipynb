{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkpNTjF/NSoB8qeBmmvQS2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greengerong/awesome-llm/blob/main/colab/llm/starcoder2_3b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 环境设置"
      ],
      "metadata": {
        "id": "0mJR3W7GFCp5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DYzpcKSFBis",
        "outputId": "9573bba1-47e9-49d5-f9c6-30b5784dbe21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 推理"
      ],
      "metadata": {
        "id": "2HCRs4iXFRAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install bitsandbytes accelerate\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# to use 4bit use `load_in_4bit=True` instead\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "checkpoint = \"bigcode/starcoder2-3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quantization_config)\n",
        "\n",
        "inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(inputs)\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHaNYIDEFSMq",
        "outputId": "432e73dc-38b1-448b-c703-109f8c5b67d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1181: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def print_hello_world():\n",
            "    print(\"Hello World\")\n",
            "\n",
            "def print_hello_\n"
          ]
        }
      ]
    }
  ]
}